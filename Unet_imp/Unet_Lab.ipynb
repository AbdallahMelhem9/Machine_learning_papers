{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayhWHLrW9eYG",
        "outputId": "7635466f-f71a-481d-ff69-adc5d68bbbd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Lab: U-Net for Oxford-IIIT Pet\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "input_size = (128, 128)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 1.1: image transform -> resize to input_size, to tensor [0,1]\n",
        "transform_img = T.Compose([\n",
        "    # TODO: fill\n",
        "])\n",
        "\n",
        "# TODO 1.2: mask transform -> resize NEAREST, keep ints, shift 1..3 -> 0..2, long\n",
        "transform_mask = T.Compose([\n",
        "    # TODO: fill\n",
        "])\n",
        "\n",
        "dataset = OxfordIIITPet(root='data', split='trainval', target_types='segmentation',\n",
        "                        transform=transform_img, target_transform=transform_mask, download=True)\n",
        "test_dataset = OxfordIIITPet(root='data', split='test', target_types='segmentation',\n",
        "                             transform=transform_img, target_transform=transform_mask, download=True)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size   = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,  num_workers=2 if device.type=='cuda' else 0)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=16, shuffle=False, num_workers=2 if device.type=='cuda' else 0)\n",
        "\n",
        "len(dataset), len(test_dataset), len(train_dataset), len(val_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mghlY2GS9ors",
        "outputId": "770c3772-7d64-46c8-85ae-240031e3e9a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 792M/792M [00:20<00:00, 38.5MB/s]\n",
            "100%|██████████| 19.2M/19.2M [00:01<00:00, 12.7MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3680, 3669, 2944, 736)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "cmap = ListedColormap(['black', 'red', 'white'])\n",
        "\n",
        "sample_img, sample_mask = train_dataset[1]\n",
        "mask_array = sample_mask.squeeze().cpu().numpy().astype(np.uint8)\n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1); plt.title(\"Input Image\"); plt.imshow(sample_img.permute(1,2,0)); plt.axis('off')\n",
        "plt.subplot(1,2,2); plt.title(\"Ground Truth Mask (0..2)\"); plt.imshow(mask_array, cmap=cmap, vmin=0, vmax=2); plt.axis('off')\n",
        "plt.show()\n",
        "#nothing to do here just run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "MKZzMXhJ9sQr",
        "outputId": "7f174517-afce-403f-af2f-0d544e670da0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'PngImageFile' object has no attribute 'squeeze'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3452987891.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msample_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmask_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PngImageFile' object has no attribute 'squeeze'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        # TODO 3.1: two 3x3 conv(+BN+ReLU) with padding=1, then Dropout\n",
        "        self.net = nn.Sequential(\n",
        "            # TODO: fill\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2)\n",
        "        self.conv = ConvBlock(in_c=out_c*2, out_c=out_c)\n",
        "    def forward(self, x, skip):\n",
        "        # TODO 3.2: upsample, size-fix to skip if needed, concat on channel dim, conv\n",
        "        # x = ...\n",
        "        # if ...: x = ...\n",
        "        # x = torch.cat([x, skip], dim=1)\n",
        "        # return self.conv(x)\n",
        "        raise NotImplementedError\n"
      ],
      "metadata": {
        "id": "S7e_ddoo9v4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNetMedium(nn.Module):\n",
        "    def __init__(self, n_classes=3, in_ch=3, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        self.enc1 = ConvBlock(in_ch, 32, p_drop)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.enc2 = ConvBlock(32, 64, p_drop)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.enc3 = ConvBlock(64, 128, p_drop)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.bott = ConvBlock(128, 256, p_drop)\n",
        "\n",
        "        self.up3 = UpBlock(256, 128)\n",
        "        self.up2 = UpBlock(128, 64)\n",
        "        self.up1 = UpBlock(64, 32)\n",
        "\n",
        "        self.outc = nn.Conv2d(32, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, return_feats=False):\n",
        "        # TODO 4.1: wire encoder, bottleneck, decoder with skips\n",
        "        # s1 = ...\n",
        "        # p1 = ...\n",
        "        # s2 = ...\n",
        "        # p2 = ...\n",
        "        # s3 = ...\n",
        "        # p3 = ...\n",
        "        # b  = ...\n",
        "        # d3 = ...\n",
        "        # d2 = ...\n",
        "        # d1 = ...\n",
        "        # out = ...\n",
        "        raise NotImplementedError\n",
        "\n",
        "model = UNetMedium(n_classes=3).to(device)\n"
      ],
      "metadata": {
        "id": "VvJsbYkE92EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiceLossMC(nn.Module):\n",
        "    def __init__(self, num_classes, smooth=1.0, eps=1e-7):\n",
        "        super().__init__()\n",
        "        self.num_classes, self.smooth, self.eps = num_classes, smooth, eps\n",
        "    def forward(self, logits, target):\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        target_oh = F.one_hot(target, num_classes=self.num_classes).permute(0,3,1,2).float()\n",
        "        dims = (0,2,3)\n",
        "        inter = (probs * target_oh).sum(dims)\n",
        "        union = probs.sum(dims) + target_oh.sum(dims)\n",
        "        dice = (2*inter + self.smooth) / (union + self.smooth + self.eps)\n",
        "        return 1 - dice.mean()\n",
        "\n",
        "w_bg, w_border, w_pet = 0.5, 2.0, 1.5\n",
        "ce_loss   = nn.CrossEntropyLoss(weight=torch.tensor([w_bg, w_border, w_pet], device=device))\n",
        "dice_loss = DiceLossMC(num_classes=3)\n",
        "\n",
        "# TODO 5.1: combine CE and Dice (e.g., CE + 0.5*Dice)\n",
        "def loss_fn(logits, y):\n",
        "    # return ...\n",
        "    raise NotImplementedError\n"
      ],
      "metadata": {
        "id": "4mxY_F_l9-W6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
        "scaler = GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    tot = 0.0\n",
        "    for x, y in train_loader:\n",
        "        x = x.to(device)\n",
        "        y = y.squeeze(1).long().to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        # TODO 6.1: AMP forward + loss + backward + step + scaler.update\n",
        "        with autocast(enabled=(device.type=='cuda')):\n",
        "            logits = model(x)\n",
        "            loss = loss_fn(logits, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        tot += loss.item()\n",
        "    return tot / max(1, len(train_loader))\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(loader):\n",
        "    model.eval()\n",
        "    tot = 0.0\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)\n",
        "        y = y.squeeze(1).long().to(device)\n",
        "        logits = model(x)\n",
        "        tot += loss_fn(logits, y).item()\n",
        "    return tot / max(1, len(loader))\n",
        "\n",
        "EPOCHS = 60\n",
        "best = 1e9\n",
        "for e in range(1, EPOCHS+1):\n",
        "    import time; t0 = time.time()\n",
        "    tr = train_epoch()\n",
        "    va = eval_epoch(val_loader)\n",
        "    print(f\"Epoch {e:02d} | train {tr:.4f} | val {va:.4f} | {time.time()-t0:.1f}s\")\n",
        "    if va < best:\n",
        "        best = va\n",
        "        torch.save(model.state_dict(), \"unet_small_best.pt\")\n"
      ],
      "metadata": {
        "id": "ZlHunFHN-BYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
        "scaler = GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    tot = 0.0\n",
        "    for x, y in train_loader:\n",
        "        x = x.to(device)\n",
        "        y = y.squeeze(1).long().to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        # TODO 6.1: AMP forward + loss + backward + step + scaler.update\n",
        "        with autocast(enabled=(device.type=='cuda')):\n",
        "            logits = model(x)\n",
        "            loss = loss_fn(logits, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        tot += loss.item()\n",
        "    return tot / max(1, len(train_loader))\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(loader):\n",
        "    model.eval()\n",
        "    tot = 0.0\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)\n",
        "        y = y.squeeze(1).long().to(device)\n",
        "        logits = model(x)\n",
        "        tot += loss_fn(logits, y).item()\n",
        "    return tot / max(1, len(loader))\n",
        "\n",
        "EPOCHS = 60\n",
        "best = 1e9\n",
        "for e in range(1, EPOCHS+1):\n",
        "    import time; t0 = time.time()\n",
        "    tr = train_epoch()\n",
        "    va = eval_epoch(val_loader)\n",
        "    print(f\"Epoch {e:02d} | train {tr:.4f} | val {va:.4f} | {time.time()-t0:.1f}s\")\n",
        "    if va < best:\n",
        "        best = va\n",
        "        torch.save(model.state_dict(), \"unet_small_best.pt\")\n"
      ],
      "metadata": {
        "id": "R1rMzOyl-Lai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "cmap = ListedColormap(['black','red','white'])\n",
        "\n",
        "def show_batch_preds(ds, n=3):\n",
        "    model.eval()\n",
        "    for i in range(n):\n",
        "        img, mask = ds[i]\n",
        "        x = img.unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            pred = logits.argmax(1).squeeze(0).cpu().numpy().astype('uint8')\n",
        "        gt = mask.squeeze().cpu().numpy().astype('uint8')\n",
        "        fig, ax = plt.subplots(1,3, figsize=(9,3))\n",
        "        ax[0].set_title(\"Input\"); ax[0].imshow(img.permute(1,2,0)); ax[0].axis('off')\n",
        "        ax[1].set_title(\"Ground Truth\"); ax[1].imshow(gt, cmap=cmap, vmin=0, vmax=2); ax[1].axis('off')\n",
        "        ax[2].set_title(\"Predicted\"); ax[2].imshow(pred, cmap=cmap, vmin=0, vmax=2); ax[2].axis('off')\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "Abxwx2FH-Nke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, matplotlib.pyplot as plt, torch\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "cmap = ListedColormap(['black','red','white'])\n",
        "\n",
        "def _to_numpy_img(x_1x3hw):\n",
        "    return x_1x3hw.detach().cpu().squeeze(0).permute(1,2,0).numpy()\n",
        "\n",
        "def _norm01(arr, eps=1e-6):\n",
        "    return (arr - arr.min()) / (arr.max() - arr.min() + eps)\n",
        "\n",
        "def visualize_feature_map(feature_tensor, title, max_channels=16, channels=None, tight=True):\n",
        "    ft = feature_tensor.detach().cpu().squeeze(0)\n",
        "    C = ft.shape[0]\n",
        "    if channels is None:\n",
        "        Cshow = min(C, max_channels); ch_idx = list(range(Cshow))\n",
        "    else:\n",
        "        ch_idx = [i for i in channels if 0 <= i < C]; Cshow = len(ch_idx)\n",
        "    cols = int(np.ceil(np.sqrt(Cshow))); rows = int(np.ceil(Cshow / cols))\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols*2, rows*2))\n",
        "    fig.suptitle(f\"{title}  (feat shape: {tuple(feature_tensor.shape)})\", fontsize=12)\n",
        "    axes = np.atleast_1d(axes).ravel()\n",
        "    for i in range(rows*cols):\n",
        "        ax = axes[i]; ax.axis('off')\n",
        "        if i < Cshow:\n",
        "            fm = ft[ch_idx[i]].numpy()\n",
        "            ax.imshow(_norm01(fm), cmap='gray'); ax.set_title(f\"ch {ch_idx[i]}\", fontsize=8)\n",
        "    if tight: plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def overlay_activation(rgb, fmap, title, reduce='mean'):\n",
        "    img = _to_numpy_img(rgb)\n",
        "    ft = fmap.detach().cpu().squeeze(0)\n",
        "    act = ft.max(0).values.numpy() if reduce=='max' else ft.mean(0).numpy()\n",
        "    act = _norm01(act)\n",
        "    plt.figure(figsize=(6,3))\n",
        "    plt.subplot(1,2,1); plt.title(\"Input\"); plt.imshow(img); plt.axis('off')\n",
        "    plt.subplot(1,2,2); plt.title(title); plt.imshow(img); plt.imshow(act, alpha=0.45, cmap='jet'); plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_concat(up, skip, concat, title, k_each=6):\n",
        "    upC, skC = up.shape[1], skip.shape[1]\n",
        "    k_up = min(k_each, upC); k_sk = min(k_each, skC)\n",
        "    panels = [up[:, i:i+1] for i in range(k_up)] + [skip[:, i:i+1] for i in range(k_sk)]\n",
        "    labels = [f\"up {i}\" for i in range(k_up)] + [f\"skip {i}\" for i in range(k_sk)]\n",
        "    k_cat = min(k_up + k_sk, concat.shape[1])\n",
        "    panels += [concat[:, i:i+1] for i in range(k_cat)]\n",
        "    labels += [f\"cat {i}\" for i in range(k_cat)]\n",
        "    n = len(panels); cols = max(k_each, 3); rows = int(np.ceil(n / cols))\n",
        "    plt.figure(figsize=(cols*2, rows*2))\n",
        "    plt.suptitle(f\"{title}\\nup:{tuple(up.shape)}  skip:{tuple(skip.shape)}  concat:{tuple(concat.shape)}\", fontsize=11)\n",
        "    for idx, tens in enumerate(panels):\n",
        "        plt.subplot(rows, cols, idx+1)\n",
        "        fm = tens.detach().cpu().squeeze().numpy()\n",
        "        plt.imshow(_norm01(fm), cmap='gray'); plt.axis('off'); plt.title(labels[idx], fontsize=8)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "def show_prediction(model, sample_img, sample_mask, show_confidence=True):\n",
        "    x = sample_img.to(device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        probs  = torch.softmax(logits, dim=1)\n",
        "        pred   = probs.argmax(1).squeeze(0).cpu().numpy().astype('uint8')\n",
        "        conf   = probs.max(1).values.squeeze(0).cpu().numpy()\n",
        "    img_np = sample_img.permute(1,2,0).cpu().numpy()\n",
        "    gt = sample_mask.squeeze().cpu().numpy().astype('uint8')\n",
        "    ncols = 4 if show_confidence else 3\n",
        "    plt.figure(figsize=(3*ncols,3))\n",
        "    plt.subplot(1,ncols,1); plt.title(\"Input\"); plt.imshow(img_np); plt.axis('off')\n",
        "    plt.subplot(1,ncols,2); plt.title(\"Ground Truth\"); plt.imshow(gt, cmap=cmap, vmin=0, vmax=2); plt.axis('off')\n",
        "    plt.subplot(1,ncols,3); plt.title(\"Predicted\"); plt.imshow(pred, cmap=cmap, vmin=0, vmax=2); plt.axis('off')\n",
        "    if show_confidence:\n",
        "        from matplotlib.colors import ListedColormap\n",
        "        plt.subplot(1,ncols,4); plt.title(\"Top-class confidence\"); plt.imshow(_norm01(conf), cmap='magma'); plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "VGvjvBOE-RA8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}